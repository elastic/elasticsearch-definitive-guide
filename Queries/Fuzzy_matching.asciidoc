[[fuzzy-matching]]
=== Fuzzy matching

Fuzzy matching is a useful techinque for matching words that have been
misspelled or have an alternate spelling. Compare:

* `bieber`    vs `beiber`     vs `beaber`
* `esophagus` vs `oesophagus` vs `Å“sophagus`

==== How do we measure fuzziness?

There are many ways to match strings approximately. We could use a
phonetic algorithm like http://en.wikipedia.org/wiki/Soundex[Soundex] or
http://en.wikipedia.org/wiki/Metaphone:[Metaphone] to find words that sound
similar. Alternatively, we could break the search terms into all possible
substrings and compare them to all of the substrings of all terms in our index.

The way that Lucene and Elasticsearch have chosen to measure fuzziness
is with the
http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance[Damerau-Levenshtein edit distance],
which counts the number of changes (_edit distance_) that have to be made to
one string in order to convert it into the target string.  A change could be
the insertion, deletion or substitution of a single character, or the
transposition of two adjacent characters.

To use our example above, `beaber` -> `bieber` has an edit distance of 2:

1. ``be**a**ber'' -> ``be**i**ber'' (replace `a` with `i`)
2. ``b**ei**ber'' -> ``b**ie**ber'' (transpose `e` and `i`)

Damerau observed that 80% of human misspellings have an edit distance of 1. In
other words, 80% of misspellings could be corrected with a single edit
to the original string.

****
The substring comparison approach, although it appears to be very
computationally heavy, actually works very well.  It does require preparation
though -- you need to have precomputed all the substrings (or _grams_) in your
index.  We will examine this further in [TODO] ADD
LINK TO NGRAMS EXAMPLE.
****

==== Generating a fuzzy query

As we have mentioned before, the use of an inverted index requires us to know
exactly which terms to look for before searching.

That means that, in order to be able to perform fuzzy matches, we need to know
ahead of time which of the terms in our index might match a fuzzy version of
our search terms. Once we have the list, we can search for all (or many)
of them and return the best matching results.

The naive way to build this term list is to calculate the edit distance between
the search term and every term in the index, discarding those whose edit
distance is too large. This may be naive, but this was the way that
the fuzzy query was implemented in Lucene up until version 4.  As a
consequence, the fuzzy query could perform badly.

==== Enter the automaton

Lucene version 4 uses a different approach -- it processes the search terms
to calculate all possible variations that have a maximum edit distance
of 2, and stores them in a very efficient structure called a
_Deterministic Finite Automaton_ (DFA). This DFA makes it possible to check
which of these terms actually exist in the term dictionary in a much more
efficient way. The result: fuzzy queries are now 100 times faster than they
used to be.

Once Elasticsearch has gatherered a list of all possible matching terms that
exist in the index, it wraps the ``most important'' terms in a
<<bool-query,boolean query>> and searches for all of them. Terms with a lower
edit distance are considered to be more relevant.

Wildcard and regular expression queries use this same DFA functionality
to improve their performance.

[[fuzzy-query]]
==== `fuzzy` query

The `fuzzy` query, like the <<prefix-query,`prefix` query>>, is a
<<term-queries,term-based query>>, which has no analysis phase.  Unlike
the other partial matching queries, it doesn't return a relevance
`_score` of 1. Instead, it tries to calculate a `_score` based upon
how similar the query term and the fuzzy term are.


[source,js]
--------------------------------------------------
{ "fuzzy": { "name": "jon" }}
--------------------------------------------------


The degree of fuzziness can be controlled by setting the `max_edits`
parameter to `1` (1 edit allowed) or `2` (1 or 2 edits allowed).

[source,js]
--------------------------------------------------
{
    "fuzzy": {
        "name": {
            "value":     "jon",
            "max_edits": 2
        }
    }
}
--------------------------------------------------



This simple looking query is rewritten into a much more complicated query
-- one for each matching term.  The way the query is rewritten can
be controlled with the `rewrite` parameter (which defaults to
`top_terms`) and which is explained in
<<multi-term-rewrite>>.

==== Fuzzy `match` queries

The <<match-query,`match` query>> (but not the `match_phrase` or
`match_phrase_prefix` queries) also supports fuzzy matching. Specify
the maximum number of edits (`1` or `2`) with the `max_edits` parameter:

[source,js]
--------------------------------------------------
{
    "match": {
        "title": {
            "query":     "Full Tekst Saerch",
            "max_edits": 2
        }
    }
}
--------------------------------------------------


The `match` query will analyze the query string before trying to ``fuzzify''
it.

==== Fuzzy `query_string` queries

The <<query-string,`query_string` query>> mini-language supports fuzzy
matching using the `~` symbol, but only on individual words. For instance:

[source,js]
--------------------------------------------------
{
    "query_string": {
        "default_field": "title",
        "query":         "Full Tekst~2 Saerch~2"
    }
}
--------------------------------------------------


[NOTE]
====
The `query_string` query does not analyze the fuzzy terms before
searching, but it does respect the `lowercase_expanded_terms` parameter.
====

==== No such thing as a free lunch

While fuzzy queries now perform very much better than they did, they still have
a cost: the DFA data structure is very large and consumes a lot of memory.
Generating it requires the creation of over 25,000 objects, all of which
then need to be garbage collected.

Before using fuzzy queries, you should:

* make sure that you have ample free memory
* restrict your use of fuzziness to a single field and a few search terms
* use the `prefix_length` parameter to reduce the number of edits that
  have to be calculated. Most people will make their typos towards the
  end of the word, not the beginning, so not fuzzifying the first few
  letters probably won't make much difference
* consider using the `max_expansions` parameter to reduce the number of
  generated fuzzy terms which will be searched on -- the default is 50
* consider using an edit distance (`max_edits`) of 1 instead of the
  default 2, as this creates a much smaller DFA
* consider using the approach outlined in the [TODO] ADD LINK TO NGRAM
  EXAMPLE instead of using fuzzy queries















