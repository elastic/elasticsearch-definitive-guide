
=== Delaying Shard Allocation

As discussed way back in <<_scale_horizontally>>, Elasticsearch will automatically
balance shards between your available nodes, both when new nodes are added and
when existing nodes leave.

Theoretically, this is the best thing to do.  We want to recover missing primaries
by promoting replicas as soon as possible.  We also want to make sure resources
are balanced evenly across the cluster to prevent hotspots.

In practice, however, immediately re-balancing can cause more problems than it solves.
For example, consider this situation:

1. Node 19 loses connectivity to your network (someone tripped on the power cable)
2. Immediately, the master notices the node departure.  It determines
what primary shards were on Node 19 and promotes the corresponding replicas around
the cluster
3. After replicas have been promoted to primary, the master begins issuing recovery
commands to rebuild the now-missing replicas.  Nodes around the cluster fire up
their NICs and start pumping shard data to each other in an attempt to get back
to green health status
4. This process will likely trigger a small cascade of shard movement, since the
cluster is now unbalanced.  Unrelated shards will be moved between hosts to accomplish
better balancing

Meanwhile, the hapless admin who kicked out the power cable plugs it back in.
Node 19 reboots and rejoins the cluster.  Unfortunately, the node is informed that
its existing data is now useless; the data being re-allocated elsewhere.
So Node 19 deletes its local data and begins recovering a different
set of shards from the cluster (which then causes a new minor re-balancing dance).

If this all sounds needless and expensive, you're right.  It is, but _only when
you know the node will be back soon_.  If Node 19 was truly gone, the above procedure
is exactly what we want to happen.

To help address these transient outages, Elasticsearch has the ability to delay
shard allocation.  This gives your cluster time to see if nodes will rejoin before
starting the re-balancing dance.

==== Changing the default delay

By default, the cluster will wait one minute to see if the node will rejoin.  If
the node rejoins before the timer expires, the rejoining node will use its existing
shards and no shard allocation occurs.

This default time can be changed either globally, or on a per-index basis, by
configuring the `delayed_timeout` setting:

[source,js]
----
PUT /_all/_settings <1>
{
  "settings": {
    "index.unassigned.node_left.delayed_timeout": "5m" <2>
  }
}
----
<1> By using the `_all` index name, we can apply this setting to all indices
in the cluster
<2> The default time is changed to 5 minutes

The setting is dynamic and can be changed at runtime.  If you would like shards to
allocate immediately instead of waiting, you can set `delayed_timeout: 0`.

NOTE: Delayed allocation won't prevent replicas from being promoted to primaries.
The cluster will still perform promotions as necessary to get the cluster back to
`yellow` status.  The allocation of the now-missing replicas will be the only process
that is delayed

==== Auto-cancellation of shard relocation

What happens if the node comes back _after_ the timeout expires, but before
the cluster has finished moving shards around?  In this case, Elasticsearch will
check to see if the on-disk data matches the current "live" data in the primary shard.
If the two shards are identical -- meaning there have been no new documents, updates
or deletes -- the master will cancel the on-going rebalancing and restore the
on-disk data.

This is done since recovery of on-disk data will always be faster
than transferring over the network, and since we can guarantee the shards are identical,
the process is a win-win.

If the shards have diverged (e.g. new documents have been indexed since the node
went down), the recovery process will continue as normal.  The rejoining node
will delete it's local, out-dated shards and obtain a new set.
