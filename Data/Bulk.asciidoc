[[bulk]]
=== Cheaper in bulk

In the same way that `mget` allows us to retrieve multiple documents at once,
the `bulk` API allows us to make multiple `create`, `index` or `delete` requests
in a single step.

This is particularly useful if you need to index a data stream such as
log events, which can be queued up and indexed in batches of 100 or 1,000.

The `bulk` request body has the following, slightly unusual, format:

    { action: { metadata }}\n
    { document            }\n
    { action: { metadata }}\n
    { document            }\n
    ...

Two important points to note about the format:

* Every line must end with a newline character `"\n"`, *including the last
  line*. These are used as markers to allow efficient line separation.

* The lines cannot contain embedded newline characters, as they would
  interfere with parsing -- that means that the JSON must *not* be
  pretty-printed.

The _action/metadata_ line specifies *what action* to do to *which document*.

The _action_ must be one of `index`, `create` or `delete`.
The _metadata_ should specify the `_index`, `_type` and `_id` of the document
to be indexed, created or deleted.

For instance, a `delete` request could look like this:

    { "delete": { "_index": "website", "_type": "blog", "_id": "123" }}

The _document_ line consists of the document `_source` itself -- the fields and values
that the document contains.  It is only required for `index` or `create`
requests, not for `delete`.

    { "index":  { "_index": "website", "_type": "blog", "_id": "123" }}
    { "title":    "My first blog post" }


If no `_id` is specified, then an ID will be auto-generated:

    { "create": { "_index": "website", "_type": "blog" }}
    { "title":    "My second blog post" }


To put it all together, a complete `bulk` request has this form:

    curl -XPOST localhost:9200/_bulk -d '
    { "delete": { "_index": "website", "_type": "blog", "_id": "123" }}
    { "index":  { "_index": "website", "_type": "blog", "_id": "123" }}
    { "title":    "My first blog post" }
    { "create": { "_index": "website", "_type": "blog" }}
    { "title":    "My second blog post" }
    '

The Elasticsearch response contains the `items` array which lists the result of
each request, in the same order as we requested them.  It also reports how many
milliseconds it `took` to perform the whole `bulk` request:

    {
      "took" : 1,
      "items" : [
        { "delete" : {
            "_index" :   "website",
            "_type" :    "blog",
            "_id" :      "123",
            "_version" : 5,
            "ok" :       true
        }},
        { "index" : {
            "_index" :   "website",
            "_type" :    "blog",
            "_id" :      "123",
            "_version" : 6,
            "ok" :       true
        }},
        { "create" : {
            "_index" :   "website",
            "_type" :    "blog",
            "_id" :      "YUYRnsC4T2yePAnMy-pqAA",
            "_version" : 1,
            "ok" :       true
        }}
      ]
    }

If any of the requests fail, then the error will be returned in `items` --
it won't stop the other requests from being processed:

    curl -XPOST localhost:9200/_bulk -d '
    { "create": { "_index": "website", "_type": "blog", "_id": "123" }}
    { "title":    "Cannot create - it already exists" }
    { "index":  { "_index": "website", "_type": "blog", "_id": "123" }}
    { "title":    "But we can update it" }
    '

In the response we can see that it failed to `create` document `123`
because it already exists. However, the subsequent `index` request, also
on document `123`, succeeded:

    {
      "took" : 1,
      "items" : [
        { "create" : {
            "_index" :   "website",
            "_type" :    "blog",
            "_id" :      "123",
            "error" :    "DocumentAlreadyExistsException[[website][4] [blog][123]:
                          document already exists]"
        }},
        { "index" : {
            "_index" :   "website",
            "_type" :    "blog",
            "_id" :      "123",
            "_version" : 7,
            "ok" :       true
        }}
      ]
    }

That also means that `bulk` requests are not atomic.  They cannot be used
to implement transactions.  Each request is processed separately, so the
failure of one request will not interfere with the others.

==== Don't repeat yourself

Perhaps you are batch indexing logging data into the same `index`, and with the
same `type`. Having to specify the same metadata for each document is a waste.

Instead, just as for the `mget` API, we can specify a default `/_index` in the
URL, or even a default `/_index/_type`.

    curl -XPOST localhost:9200/website/_bulk -d '
    { "index": { "_type": "log" }}
    { "event": "User logged in" }
    '

You can still override the `_index` and `_type` in the metadata line, but it
will use the URL defaults if no override is specified:

    curl -XPOST localhost:9200/website/log/_bulk -d '
    { "index": {}}
    { "event": "User logged in" }
    { "index": { "_type": "blog" }
    { "title": "Overriding the default type" }}
    '

Unlike the `mget` API, the `_type` parameter is not optional in `bulk` API.
It must be specified, either in the URL or in the metadata.

==== Conflict control

We can use `_version` numbers to avoid overwriting data in the same
way as do for single `index` or `delete` requests (see <<version-control>>).

The `_version` number must be specified in the metadata:

    curl -XPOST localhost:9200/website/blog/_bulk -d '
    { "create": { "_id": "125" }}
    { "title":    "Create a new blog post, with version 1" }
    { "index":  { "_id": "125", "_version": 1 }}
    { "title":    "This update succeeds" }
    { "index":  { "_id": "125", "_version": 1 }}
    { "title":    "This update fails with a Conflict error" }
    '

The metadata also understands the `_version_type` parameter, if you wish
to use `external` version numbers.

==== How big is too big?

There is an optimal size of `bulk` request. Above that size, performance
no longer improves and may even drop off. Also, the entire bulk request
needs to be loaded into memory by the node which receives our request,
so the bigger the request, the less memory available for other requests.

The optimal size, however, is not a fixed number. It depends entirely on your
hardware, your document size and complexity, and your indexing and search
load.  Fortunately, it is easy to find the _sweetspot_:

Try indexing typical documents in batches of increasing size. When
performance starts to drop, your batch size is too big.

A good place to start is with batches of between 1,000 and 5,000 documents or,
if your documents are very large, with even smaller batches.

==== Why the funny format?

You may have asked yourself: ``Why does the `bulk` API require the funny format
with the newline characters, instead of just sending the requests wrapped in
a JSON array, like the `mget` API?''

To answer this, we need to explain a little background:

Documents are stored and indexed in shards. An index is just a logical namespace
which points to one or more shards.  On top of that, a cluster may contain
multiple indices. Elasticsearch uses the `_index`, `_type` and `_id` of the
document to determine which shard it should belong to.

If you are running a cluster with more than one node, then it is likely that
these shards will be allocated to different nodes. Each _action_ inside a `bulk`
request needs to be forwarded to the correct shard on the correct node.

If the individual requests were wrapped up in a JSON array, that would mean
that we would need to:

 * parse the JSON into an array (including the document data, which
   can be very large)
 * look at each request to determine which shard it should go to
 * create an array of requests for each shard
 * serialize these arrays into the internal transport format
 * send the requests to each shard

It would work, but would need a lot of RAM to hold copies of essentially
the same data, and would create many more data structures that the JVM
would have to spend time garbage collecting.

Instead, Elasticsearch reaches up into the networking buffer, where
the raw request has been received and reads the data directly. It uses the
newline characters to identify and parse just the small _action/metadata_ lines
in order to decide which shard should handle each request.

These raw requests are forwarded directly to the correct shard. There
is no redundant copying of data, no wasted data structures. The entire
request process is handled in the smallest amount of memory possible.

This is a good example of just how much thought and effort the Elasticsearch
authors have put in to optimizing performance.

