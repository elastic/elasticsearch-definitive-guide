
Index-time partial matching is a simple concept: index partial fragments of each term.  At query time, you no longer perform partial matches, you simply perform regular searches.

Creating these fragments is accomplished using the `ngram` token filter. This filter accepts a token and then slides a "window" of size N over the length of the token.  At each position, a new token of size N is extracted.  This sounds complicated, but it is very simple.

Given the word "pancake" and a size of three, we generate the following new tokens:

[source,js]
--------------------------------------------------
"pan", "anc", "nca", "cak", "ake"
--------------------------------------------------


The minimum and maximum size of the ngram is configurable.  If you set min = 2 and max = 3, you would generate all combinations between two and three:

[source,js]
--------------------------------------------------
"pa", "pan", "an", "anc", ....
--------------------------------------------------


While this appears to be useful, in practice it does not help search relevance and simply bloats the index (and potentially slows everything down).  Generally speaking, spending time to find the optimal n-gram size is more productive than generating all ngrams between n and n+10.

==== Searching your ngrams

Ok, so you have a big pile of ngrams in your index, what is the best way to search for partial matches?  It's actually pretty easy: the match query is probably your best bet.

By default, the match query will analyze your input with the analyzer of the field.  Let's imagine that the search input is "pankake", it will be analyzed into the following tokens:

[source,js]
--------------------------------------------------
"pan", "ank", "nka", "kak", "ake"
--------------------------------------------------


And, if you'll look back at the tokenization of "pancake", the two terms have two tokens in common.  The search query will turn up "pancake" even though it was originally mispelled

You can imagine this approach being applied to entire phrases.  The more the two phrases have in common, the more ngram tokens will match and thus the higher score.

.Perfect for mispellings?
****
At first glance, this seems like a great way to allow "fuzzy" matching that tolerates mispellings.  It will work, but there are better ways to accomplish that task.

In particular, there are three differen Suggesters built into Elasticsearch which will be much better for spelling correction.
****